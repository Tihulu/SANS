{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe95a38",
   "metadata": {},
   "source": [
    "# SANS: Using CNN to predict model ID\n",
    "We will utilize the zenodo [repository](https://zenodo.org/records/10119316) as our data for this project. There is no need to locally download the data as we will be using the library `fsspec` to work directly with the link of the `.h5` files. This will not load the data from the remote files into the user's working memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a5c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "%colors lightbg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60faf40",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import fsspec\n",
    "import torch\n",
    "from torch import nn\n",
    "import pickle\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc108ad0",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"https://zenodo.org/records/10119316/files/train.h5\"\n",
    "test_url = \"https://zenodo.org/records/10119316/files/test.h5\"\n",
    "val_url = \"https://zenodo.org/records/10119316/files/val.h5\"\n",
    "remote_f = fsspec.open(train_url, mode=\"rb\")\n",
    "if hasattr(remote_f, \"open\"):\n",
    "    remote_f = remote_f.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7934a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, h5_path, transforms=None):\n",
    "        self.h5_file = h5py.File(h5_path, \"r\")\n",
    "        self.transform = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.h5_file[\"data\"][index]\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return (\n",
    "            sample,\n",
    "            int(self.h5_file[\"target\"][index]),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.h5_file[\"target\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb701191",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Resize((180, 180), antialias=True),\n",
    "        torch.nn.ReLU(inplace=True),  # remove negative values if any\n",
    "        torchvision.transforms.Lambda(lambda x: torch.log(x + 1.0)),\n",
    "        torchvision.transforms.Lambda(\n",
    "            lambda x: x / torch.max(x) if torch.max(x) > 0 else x\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_data = H5Dataset(remote_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1340b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a437e4c",
   "metadata": {},
   "source": [
    "### Plotting a typical 2D intensity array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b6ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 537\n",
    "psd_array = train_data[idx][0]\n",
    "target = train_data[idx][1]\n",
    "plt.figure()\n",
    "plt.imshow(psd_array)\n",
    "plt.title(target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092a0ce",
   "metadata": {},
   "source": [
    "### exploring the model names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6d8d9",
   "metadata": {},
   "source": [
    "Manually add the `.pkl` files inside the directory `sas_helper/` as some of the files are too large to be uploaded on a GitHub repo. These files have been purposefully added to `.gitignore` due to this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sas_helper/model_names.pkl\", \"rb\") as pf:    # \n",
    "    model_names = pickle.load(pf)\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f50b62",
   "metadata": {},
   "source": [
    "In our case, the model name is given as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0941813",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names[39]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ec05a",
   "metadata": {},
   "source": [
    "### pytorch dataloader\n",
    "\n",
    "Here is a code template you could use to put this dataset in a dataloader for `pytorch`, which could directly go in a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=32, # play with this also\n",
    "    num_workers=2, #modify to your cpus available!\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4662ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23804d5c",
   "metadata": {},
   "source": [
    "### loading instrument parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_params = {}\n",
    "for partition in [\"test\", \"train\", \"val\"]:\n",
    "    with open(f\"sas_helper/inst_params_{partition}.pkl\", \"rb\") as pf:\n",
    "        inst_params[partition] = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2203eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_params['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529018a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_params['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10016955",
   "metadata": {},
   "source": [
    "We loaded all the instrument parameters inside the `inst_params` dictionary, with the corresponding partition as key. The 10 parameters that are one-hot encoded are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_params_names  = ['Lam_4.5', 'Lam_6.0', 'zdepth_0.001', 'zdepth_0.002', 'InstSetting_1', 'InstSetting_2', 'InstSetting_3', 'SlitSetting_1', 'SlitSetting_2', 'SlitSetting_3']\n",
    "print(inst_params_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae1c200",
   "metadata": {},
   "source": [
    "- Lam = Lambda, wavelength of monochromatized neutrons in Angstroms. Two possible values [4.5, 6.0]\n",
    "- zdepth = sample thickness, Two possible values [0.001, 0.002]\n",
    "- InstSetting = Instrument setting. 3 possible values [1, 2, 3]\n",
    "- SlitSetting = slit (collimation) setting, 3 possible values [1, 2, 3]\n",
    "\n",
    "Again, variables are one-hot encoded. Order of columns matters. This means that case 0 (described above) was measured with a wavelength of 4.5, a sample thickness of 0.001, and a slit setting of 2. (check)\n",
    "\n",
    "This matrix can be used as **input features** for the regression or classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6934fb2",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7900072",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    device=torch.device(\"cuda:0\")   # Works for NVidia and AMD GPUs\n",
    "elif torch.mps.device_count() > 0:\n",
    "    device=torch.device(\"mps:0\")    # Metal Performance Shaders backend for Mac\n",
    "else:\n",
    "    device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366090c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 64, 2)\n",
    "        self.fc1 = nn.Linear(64 * 2 * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 46)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x))) \n",
    "        x = torch.flatten(x, 1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = model()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e30f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = list(model.parameters())\n",
    "print(model)\n",
    "print(len(weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = model(train_data[0][0])\n",
    "output = model(torch.randn(2, 3, 11, 11))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(images)\n",
    "loss = loss_func(outputs, labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved_model.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
